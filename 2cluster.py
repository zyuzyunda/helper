from sentence_transformers import SentenceTransformer
import umap
import hdbscan
import json
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from collections import defaultdict
import os


# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
GROUPED_SKILLS_PATH = "/Users/macbook/Projects/helper/app/data/grouped_skills_named.json"

# –ó–∞–≥—Ä—É–∑–∫–∞
with open(GROUPED_SKILLS_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)
group_names = list(data.keys())

# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
embeddings = model.encode(group_names, show_progress_bar=True)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ UMAP
scaler = StandardScaler()
scaled = scaler.fit_transform(embeddings)
umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='cosine', random_state=42)
proj = umap_model.fit_transform(scaled)

# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
clusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean')
labels = clusterer.fit_predict(proj)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(12, 6))
plt.title("–í–µ—Ä—Ö–Ω–µ—É—Ä–æ–≤–Ω–µ–≤–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –≥—Ä—É–ø–ø –Ω–∞–≤—ã–∫–æ–≤ (UMAP + HDBSCAN)")
plt.scatter(proj[:, 0], proj[:, 1], c=labels, cmap='tab20', s=8)
plt.colorbar(label="Cluster ID")
plt.grid(True)
plt.tight_layout()
plt.show()

# –ü–µ—á–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
cluster_map = defaultdict(list)
for name, label in zip(group_names, labels):
    cluster_map[label].append(name)

for cluster_id, names in cluster_map.items():
    print(f"\nüß† –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ‚Äî {len(names)} –≥—Ä—É–ø–ø:")
    for name in names[:10]:
        print(f"  ‚Ä¢ {name}")
    if len(names) > 10:
        print(f"  ...–∏ –µ—â—ë {len(names) - 10} –≥—Ä—É–ø–ø")

print("\nüîç –ù–∞—á–∏–Ω–∞–µ–º –≤–ª–æ–∂–µ–Ω–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –≤–Ω—É—Ç—Ä–∏ –≤–µ—Ä—Ö–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")

for cluster_id, group_titles in cluster_map.items():
    print(f"\nüì¶ –í–ª–æ–∂–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –≤–µ—Ä—Ö–Ω–∏–π –∫–ª–∞—Å—Ç–µ—Ä {cluster_id} ({len(group_titles)} –≥—Ä—É–ø–ø)")

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–∞–≤—ã–∫–∏ –∏–∑ –≤—Ö–æ–¥—è—â–∏—Ö –≥—Ä—É–ø–ø
    raw_skills = []
    for group in group_titles:
        raw_skills.extend(data.get(group, []))
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    raw_skills = list(set(raw_skills))
    if len(raw_skills) < 5:
        print(f"  ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ({len(raw_skills)}). –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
        continue

    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
    skill_emb = model.encode(raw_skills, show_progress_bar=False)

    # UMAP
    scaled_skills = scaler.fit_transform(skill_emb)
    umap_skills = umap_model.fit_transform(scaled_skills)

    # HDBSCAN –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã
    inner_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')
    inner_labels = inner_clusterer.fit_predict(umap_skills)

    # –°–±–æ—Ä –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    inner_map = defaultdict(list)
    for skill, label in zip(raw_skills, inner_labels):
        inner_map[label].append(skill)

    # –í—ã–≤–æ–¥
    for inner_id, skills in inner_map.items():
        print(f"\n  üß© –ü–æ–¥–∫–ª–∞—Å—Ç–µ—Ä {inner_id} ‚Äî {len(skills)} –Ω–∞–≤—ã–∫–æ–≤:")
        for s in skills[:8]:
            print(f"   ‚Ä¢ {s}")
        if len(skills) > 8:
            print(f"   ...–∏ –µ—â—ë {len(skills) - 8}")


import os

# –ü—É—Ç—å –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏
output_json_path = "./cluster_with_subclusters.json"
output_viz_dir = "./viz_clusters"
os.makedirs(output_viz_dir, exist_ok=True)

# –°–±–æ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
cluster_results = {}

for cluster_id, group_titles in cluster_map.items():
    cluster_key = str(cluster_id)

    raw_skills = []
    for group in group_titles:
        raw_skills.extend(data.get(group, []))

    raw_skills = list(set(raw_skills))
    if len(raw_skills) < 5:
        continue

    skill_emb = model.encode(raw_skills, show_progress_bar=False)
    scaled_skills = scaler.fit_transform(skill_emb)
    umap_skills = umap_model.fit_transform(scaled_skills)
    inner_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')
    inner_labels = inner_clusterer.fit_predict(umap_skills)

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –Ω–∞–≤—ã–∫–æ–≤ –ø–æ –ø–æ–¥–∫–ª–∞—Å—Ç–µ—Ä–∞–º
    subclusters = defaultdict(list)
    for skill, label in zip(raw_skills, inner_labels):
        subclusters[str(label)].append(skill)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–µ–∑ —Å—Ç–∞—Ä—ã—Ö –≥—Ä—É–ø–ø
    cluster_results[cluster_key] = {
        "cluster_name": f"Cluster {cluster_key}",  # –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å
        "subclusters": {
            str(sub_id): sorted(skills)
            for sub_id, skills in subclusters.items()
            if sub_id != -1 and len(skills) > 0
        }
    }


    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 5))
    scatter = plt.scatter(umap_skills[:, 0], umap_skills[:, 1], c=inner_labels, cmap="tab10", s=10)
    plt.title(f"–ü–æ–¥–∫–ª–∞—Å—Ç–µ—Ä—ã –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster_id}")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(output_viz_dir, f"cluster_{cluster_id}.png"))
    plt.close()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
with open("/Users/macbook/Projects/helper/app/data/final_clusters.json", "w", encoding="utf-8") as f:
    json.dump(cluster_results, f, ensure_ascii=False, indent=2)

print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_json_path}")
print(f"üñºÔ∏è –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_viz_dir}")
